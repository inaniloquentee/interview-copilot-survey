# app.py
import streamlit as st
import pandas as pd
import numpy as np
from backend import InterviewCopilotBackend
import config

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(page_title="Smart Interview Copilot", layout="wide", page_icon="ğŸš€")

# åˆå§‹åŒ– Backend (ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤åŠ è½½æ¨¡å‹)
@st.cache_resource
def get_backend():
    return InterviewCopilotBackend()

backend = get_backend()

# --- ä¾§è¾¹æ ï¼šå…¨å±€è®¾ç½® ---
with st.sidebar:
    st.title("âš™ï¸ å¤‡è€ƒè®¾ç½®")
    user_mode = st.radio(
        "å½“å‰å¤‡è€ƒæ¨¡å¼",
        ("steady", "urgent"),
        format_func=lambda x: "ğŸ“… æŒ‰éƒ¨å°±ç­ (ç¨³æ‰ç¨³æ‰“)" if x == "steady" else "ğŸ”¥ ç«çƒ§çœ‰æ¯› (åªçœ‹é«˜é¢‘)"
    )
    st.info(f"å½“å‰ç®—æ³•æƒé‡:\nAlpha(é‡è¦æ€§): {config.MODE_CONFIG[user_mode]['alpha']}\nBeta(è–„å¼±é¡¹): {config.MODE_CONFIG[user_mode]['beta']}")
    
    st.divider()
    st.write("ğŸ“Š **æ€»è¿›åº¦æ¦‚è§ˆ**")
    # æ¨¡æ‹Ÿæ•°æ®
    st.progress(0.3, text="æ•´ä½“æŒæ¡ç‡ 30%")

# --- ä¸»ç•Œé¢ Tabs ---
tab1, tab2, tab3 = st.tabs(["ğŸ“¤ æ™ºèƒ½å¯¼å…¥ (Ingestion)", "ğŸ“ˆ å¤ä¹ çœ‹æ¿ (Dashboard)", "ğŸ¤– æ¨¡æ‹Ÿé¢è¯• (Mock Interview)"])

# === Tab 1: æ•°æ®å¯¼å…¥ ===
with tab1:
    st.header("çŸ¥è¯†åº“æ‰©å……")
    st.markdown("ä¸Šä¼ é¢è¯•é¢˜æˆªå›¾ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨è¯†åˆ«å¹¶æ¸…æ´—å…¥åº“ã€‚")
    
    uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡ä»¶", type=['png', 'jpg', 'jpeg', 'pdf'])
    
    if uploaded_file is not None:
        file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
        
        # æ˜¾ç¤ºå›¾ç‰‡
        st.image(uploaded_file, caption="åŸå§‹å›¾ç‰‡", width=300)
        
        if st.button("ğŸš€ å¼€å§‹ AI è¯†åˆ«ä¸æå–"):
            with st.spinner("PaddleOCR æ­£åœ¨è¯†åˆ«æ–‡å­—..."):
                raw_text = backend.ocr_process(uploaded_file.getvalue())
                st.session_state['raw_text'] = raw_text
                
            st.success("OCR è¯†åˆ«å®Œæˆï¼")
            with st.expander("æŸ¥çœ‹è¯†åˆ«åŸæ–‡"):
                st.text(raw_text)
            
            with st.spinner("ERNIE æ­£åœ¨ç”Ÿæˆç»“æ„åŒ–é¢˜åº“..."):
                qa_list = backend.extract_knowledge(raw_text)
                st.session_state['qa_list'] = qa_list 

        if 'qa_list' in st.session_state and st.session_state['qa_list']:
            st.write("### ğŸ§  æå–ç»“æœé¢„è§ˆ")
            df = pd.DataFrame(st.session_state['qa_list'])
            st.dataframe(df, use_container_width=True)
            
            if st.button("ğŸ’¾ ç¡®è®¤å…¥åº“"):
                count = backend.save_to_db(st.session_state['qa_list'])
                st.toast(f"æˆåŠŸå­˜å…¥ {count} é“é¢è¯•é¢˜ï¼", icon="âœ…")
                del st.session_state['qa_list']

# === Tab 2: å¤ä¹ çœ‹æ¿ ===
with tab2:
    st.header("ğŸ¯ ä»Šæ—¥æ™ºèƒ½æ¨è")
    st.caption(f"åŸºäºã€Œ{user_mode}ã€æ¨¡å¼ç”Ÿæˆçš„åŠ¨æ€ä¼˜å…ˆçº§åˆ—è¡¨")
    
    # è·å–æ¨è
    recommendations = backend.get_recommendations(user_mode)
    
    if not recommendations:
        st.info("é¢˜åº“ä¸ºç©ºï¼Œè¯·å…ˆå» Tab 1 ä¸Šä¼ èµ„æ–™ï¼")
    else:
        for idx, item in enumerate(recommendations):
            col1, col2, col3 = st.columns([1, 4, 2])
            with col1:
                st.metric("æ¨èåˆ†", f"{item['algo_score']:.2f}")
            with col2:
                st.subheader(f"{idx+1}. {item['question']}")
                # å…¼å®¹ä¸åŒç±»å‹çš„é‡è¦æ€§å¾—åˆ†
                importance_score = int(item['importance']) if str(item['importance']).isdigit() else 5
                st.caption(f"æ ‡ç­¾: {item['tags']} | è€ƒé¢‘: {'â­' * importance_score}")
            with col3:
                status_color = "red" if item['mastery_score'] < 0.5 else "green"
                st.markdown(f"æŒæ¡åº¦: :{status_color}[{item['mastery_score']*100:.0f}%]")
                
                # ğŸ’¡ æ ¸å¿ƒä¿®å¤ï¼šç§»é™¤ switch_pageï¼Œæ”¹ç”¨çŠ¶æ€é€šçŸ¥
                if st.button("å¼€å§‹å¤ä¹ ", key=f"btn_{item['id']}"):
                    # å¦‚æœæ¢äº†ä¸€é“æ–°é¢˜ï¼Œå…ˆæ¸…ç©ºä¸Šä¸€æ¬¡çš„èŠå¤©è®°å½•
                    if st.session_state.get('current_q', {}).get('id') != item['id']:
                        st.session_state.messages = []
                        
                    st.session_state['current_q'] = item
                    # å¼¹å‡ºå³ä¸‹è§’æç¤ºï¼Œå¼•å¯¼ç”¨æˆ·ç‚¹å‡» Tab 3
                    st.toast("âœ… é¢˜ç›®å·²é”å®šï¼è¯·ç‚¹å‡»ä¸Šæ–¹ã€ŒğŸ¤– æ¨¡æ‹Ÿé¢è¯•ã€æ ‡ç­¾é¡µå¼€å§‹ä½œç­”", icon="ğŸ‘‰")

# === Tab 3: æ¨¡æ‹Ÿé¢è¯• ===
with tab3:
    st.header("ğŸ¤– AI é¢è¯•å®˜")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é€‰ä¸­çš„é¢˜ç›®
    if 'current_q' not in st.session_state:
        st.info("ğŸ’¡ è¯·å…ˆä»ã€Œå¤ä¹ çœ‹æ¿ã€é€‰æ‹©ä¸€é“é¢˜å¼€å§‹ã€‚")
    else:
        q = st.session_state['current_q']
        st.info(f"**æ­£åœ¨è€ƒå¯Ÿ**ï¼š{q['question']}")
        
        # èŠå¤©ç•Œé¢åˆå§‹åŒ–
        if "messages" not in st.session_state:
            st.session_state.messages = []

        # æ¸²æŸ“å†å²å¯¹è¯
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        # æ¥æ”¶ç”¨æˆ·è¾“å…¥
        if user_input := st.chat_input("è¯·è¾“å…¥ä½ çš„å›ç­”..."):
            st.session_state.messages.append({"role": "user", "content": user_input})
            with st.chat_message("user"):
                st.markdown(user_input)

            # ğŸ’¡ æ ¸å¿ƒå‡çº§ï¼šçœŸæ­£è°ƒç”¨æ–‡å¿ƒä¸€è¨€ API è¿›è¡Œæ™ºèƒ½æ‰“åˆ†å’Œç‚¹è¯„
            with st.chat_message("assistant"):
                with st.spinner("é¢è¯•å®˜æ­£åœ¨ä»”ç»†è¯„ä¼°ä½ çš„å›ç­”..."):
                    eval_prompt = f"""
                    ä½ ç°åœ¨æ˜¯æŠ€æœ¯é¢è¯•å®˜ã€‚
                    å½“å‰é¢è¯•é¢˜ç›®ï¼š{q['question']}
                    è¯¥é¢˜ç›®çš„æ ‡å‡†ç­”æ¡ˆå‚è€ƒï¼š{q['answer']}
                    
                    å€™é€‰äººçš„å›ç­”ï¼š{user_input}
                    
                    è¯·ä½ æ‰®æ¼”ä¸¥æ ¼ä½†å®¢è§‚çš„é¢è¯•å®˜ï¼Œç»™å‡ºä»¥ä¸‹å†…å®¹ï¼š
                    1. ç»¼åˆæ‰“åˆ†ï¼ˆæ»¡åˆ† 100 åˆ†ï¼Œè¯·åŠ ç²—æ˜¾ç¤ºï¼Œå¦‚ **è¯„åˆ†ï¼š85åˆ†**ï¼‰
                    2. ç‚¹è¯„ï¼ˆæŒ‡å‡ºå€™é€‰äººå›ç­”æ­£ç¡®çš„åœ°æ–¹ï¼Œä»¥åŠæ¬ ç¼ºæˆ–ä¸å‡†ç¡®çš„åœ°æ–¹ï¼‰
                    3. æ”¹è¿›å»ºè®®ï¼ˆç»™å‡ºæ›´å®Œå–„çš„è¡¨è¿°æ–¹å¼ï¼‰
                    """
                    
                    try:
                        # å¤ç”¨ backend é‡Œçš„ ai_client å‘èµ·å¯¹è¯
                        response_obj = backend.ai_client.chat.completions.create(
                            model="ernie-4.5-turbo-128k",  
                            messages=[{"role": "user", "content": eval_prompt}],
                            temperature=0.3
                        )
                        response = response_obj.choices[0].message.content
                    except Exception as e:
                        response = f"**è¯„åˆ†å¤±è´¥**\n\nAI æ¥å£è°ƒç”¨å‡ºé”™: {e}"
                        
                    st.markdown(response)
            
            st.session_state.messages.append({"role": "assistant", "content": response})